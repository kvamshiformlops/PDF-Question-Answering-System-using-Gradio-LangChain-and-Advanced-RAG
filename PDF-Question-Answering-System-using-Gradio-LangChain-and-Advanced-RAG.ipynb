{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamshi krishna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamshi krishna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.38.1, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n",
      "c:\\Users\\vamshi krishna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\vamshi krishna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vamshi krishna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self):\n",
    "        self.vector_store = None\n",
    "        self.text_chunks = []\n",
    "        self.pdf_files = None  # To store uploaded PDF files\n",
    "\n",
    "    def sanitize_text(self, text):\n",
    "        return text.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "\n",
    "    def get_pdf_text(self, pdf_docs):\n",
    "        text = \"\"\n",
    "        try:\n",
    "            for pdf in pdf_docs:\n",
    "                pdf_reader = PdfReader(pdf.name)\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += self.sanitize_text(page_text)\n",
    "            if not text.strip():\n",
    "                return \"Error: No text found in PDFs.\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "    def get_text_chunks(self, text):\n",
    "        try:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            if not chunks:\n",
    "                return \"Error: Failed to split text into chunks.\"\n",
    "            self.text_chunks = chunks\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            return f\"Error splitting text: {str(e)}\"\n",
    "\n",
    "    def create_vector_store(self, text_chunks):\n",
    "        try:\n",
    "            embedding_func = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "            self.vector_store = FAISS.from_texts(text_chunks, embedding=embedding_func)\n",
    "            return \"Vector store created successfully.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error creating vector store: {str(e)}\"\n",
    "\n",
    "    def get_conversational_chain(self):\n",
    "        try:\n",
    "            prompt_template = \"\"\"\n",
    "            Answer the question as detailed as possible based strictly on the content of the uploaded document(s).\n",
    "            The answer should contain a minimum of 150 words if it is in the context.\n",
    "            If the answer is not directly found in the document, respond with 'The answer is not in the provided context.'\n",
    "            Context: {context}\n",
    "            Question: {question}\n",
    "\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            llm = Ollama(model=\"llama3\", temperature=0.5)\n",
    "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "            chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "            return chain\n",
    "        except Exception as e:\n",
    "            return f\"Error creating conversational chain: {str(e)}\"\n",
    "\n",
    "    def user_input(self, user_question):\n",
    "        try:\n",
    "            if self.vector_store is None:\n",
    "                return \"Error: No vector store found. Please upload and process PDFs first.\"\n",
    "\n",
    "            docs = self.vector_store.similarity_search(user_question)\n",
    "            if not docs:\n",
    "                return \"Error: No similar documents found.\"\n",
    "\n",
    "            chain = self.get_conversational_chain()\n",
    "            if isinstance(chain, str) and chain.startswith(\"Error\"):\n",
    "                return chain\n",
    "\n",
    "            context = \"\\n\".join([self.sanitize_text(doc.page_content) for doc in docs])\n",
    "            response = chain({\"input_documents\": docs, \"question\": user_question})\n",
    "            if not response or \"output_text\" not in response:\n",
    "                return \"Error: Failed to generate response.\"\n",
    "\n",
    "            return response[\"output_text\"]\n",
    "        except Exception as e:\n",
    "            return f\"Error processing user input: {str(e)}\"\n",
    "\n",
    "    def process_pdfs(self, pdf_files):\n",
    "        self.pdf_files = pdf_files\n",
    "        combined_text = self.get_pdf_text(pdf_files)\n",
    "        if isinstance(combined_text, str) and combined_text.startswith(\"Error\"):\n",
    "            return combined_text\n",
    "\n",
    "        text_chunks = self.get_text_chunks(combined_text)\n",
    "        if isinstance(text_chunks, str) and text_chunks.startswith(\"Error\"):\n",
    "            return text_chunks\n",
    "\n",
    "        vector_store_status = self.create_vector_store(text_chunks)\n",
    "        if isinstance(vector_store_status, str) and vector_store_status.startswith(\"Error\"):\n",
    "            return vector_store_status\n",
    "\n",
    "        return \"Processing completed.\"\n",
    "\n",
    "# Create an instance of PDFProcessor\n",
    "pdf_processor = PDFProcessor()\n",
    "\n",
    "# Gradio interface for uploading and processing PDF files with a \"Process\" button\n",
    "process_interface = gr.Interface(\n",
    "    fn=pdf_processor.process_pdfs,\n",
    "    inputs=gr.File(label=\"Upload PDF Files\", file_count=\"multiple\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Process PDFs\",\n",
    "    description=\"Upload multiple PDF files, then click 'Process' to analyze.\"\n",
    ")\n",
    "\n",
    "# Interface for asking questions\n",
    "question_interface = gr.Interface(\n",
    "    fn=pdf_processor.user_input,\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Ask Questions\",\n",
    "    description=\"Ask questions based on the processed PDF files.\"\n",
    ")\n",
    "\n",
    "# Combining interfaces into a tabbed interface with only two tabs\n",
    "app = gr.TabbedInterface(\n",
    "    [process_interface, question_interface], \n",
    "    tab_names=[\"Process PDFs\", \"Ask Questions\"]\n",
    ")\n",
    "\n",
    "app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
